{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separates spectrograms into training and test set, then convert them to Tensorflow-friendly numpy arrays to be fed into our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "image_width = 50  #pixel width ie. time bins\n",
    "image_height = 37 #pixel height ie. frequency bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions to save/load numpy arrays to/from file\n",
    "\n",
    "def save_sets(sets,name):\n",
    "    \"\"\"Writes the data array to .npy file. Can be loaded using load_set.\n",
    "    sets: arrays to be saved. can take a list\n",
    "    name: string to name the file. follow same order as in sets \n",
    "    \"\"\" \n",
    "    ind = 0\n",
    "    for x in sets:\n",
    "        np.save('{}.npy'.format(name[ind]), x)\n",
    "        ind += 1\n",
    "\n",
    "def load_set(sets):\n",
    "    \"\"\"Load existing data arrays from .npy files. Use if have preexisting data or when you don't to reshuffle the dataset\"\"\"\n",
    "    return np.load('{}.npy'.format(sets))\n",
    "\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To measure time taken\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we load spectrograms from file and index by class into python dics. Indices can be saved to be reused \n",
    "#!!If already have an array to load, skip and directly load that instead (further down)\n",
    "import random\n",
    "\n",
    "def get_subdirs(a_dir):\n",
    "    \"\"\" Returns a list of sub directory names in a_dir \"\"\" \n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if (os.path.isdir(os.path.join(a_dir, name)) and not (name.startswith('.')))]\n",
    "\n",
    "def fetch_files(spec_dir):\n",
    "    \"\"\"Returns a python dictionary of classes as keys and the (short)filename as values\"\"\"\n",
    "    data_files = {} \n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        data_files[i] = class_files #each spectrogram is associated with its class via dictionary key\n",
    "        i += 1\n",
    "    return data_files, spec_dir\n",
    "\n",
    "def getFold(string):\n",
    "    \"\"\"get fold no. from ESC-50 dataset using the filename. Labels #1-5\"\"\"\n",
    "    label_pos = string.index(\"-\")\n",
    "    return string[label_pos-1]\n",
    "\n",
    "data, data_dir = fetch_files(\"C:/Users/Huz/Documents/python_scripts/ESC50_multitask/ESC-50-cqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the test from training set by proportion specified before (in get_ind).\n",
    "#Can save the datasets as numpy arrays for future use.\n",
    "#!!If already have an array to load, skip and directly load that instead (further down)\n",
    "import collections as c\n",
    "\n",
    "fold1,fold2,fold3,fold4,fold5 = [],[],[],[],[] #create 5 folds to be used for cross-val\n",
    "fold1_L,fold2_L,fold3_L,fold4_L,fold5_L = [],[],[],[],[] #mirror with 5 folds for labels\n",
    "\n",
    "def create_folds(data_files,spec_dir):\n",
    "    \"\"\"\"Formats test and training data and labels in numpy arrays.\"\"\"\n",
    "\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    \n",
    "    for key, value in data_files.items():\n",
    "        for filename in value:\n",
    "            try:\n",
    "                image_data = np.array(ndimage.imread(spec_dir + \"/\" + class_folders[key] + \"/\" \n",
    "                                                     + filename).astype(float)) - 0.5\n",
    "                if image_data.shape != (image_height, image_width):\n",
    "                    raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "                \n",
    "                fold = getFold(filename) #use Piczak's prearranged folds\n",
    "                if int(fold) == 1:\n",
    "                    fold1.append(image_data)\n",
    "                    fold1_L.append(key)\n",
    "                elif int(fold) == 2:\n",
    "                    fold2.append(image_data)\n",
    "                    fold2_L.append(key)\n",
    "                elif int(fold) == 3:\n",
    "                    fold3.append(image_data)\n",
    "                    fold3_L.append(key)\n",
    "                elif int(fold) == 4:\n",
    "                    fold4.append(image_data)\n",
    "                    fold4_L.append(key)\n",
    "                elif int(fold) == 5:\n",
    "                    fold5.append(image_data)\n",
    "                    fold5_L.append(key)\n",
    "            except IOError as e:\n",
    "                print('Could not read:', data_files[data_class][j], ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "def shuffle(dataset, labels):\n",
    "    \"\"\"Randomizes order of elements in input arrays\"\"\"\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "create_folds(data,data_dir)\n",
    "\n",
    "fold1, fold1_L = shuffle(np.array(fold1),np.array(fold1_L))\n",
    "fold2, fold2_L = shuffle(np.array(fold1),np.array(fold1_L))\n",
    "fold3, fold3_L = shuffle(np.array(fold1),np.array(fold1_L))\n",
    "fold4, fold4_L = shuffle(np.array(fold1),np.array(fold1_L))\n",
    "fold5, fold5_L = shuffle(np.array(fold1),np.array(fold1_L))\n",
    "\n",
    "# print(c.Counter(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1 size: (400, 37, 50, 1) (400, 50)\n",
      "Fold2 size: (400, 37, 50, 1) (400, 50)\n",
      "Fold3 size: (400, 37, 50, 1) (400, 50)\n",
      "Fold4 size: (400, 37, 50, 1) (400, 50)\n",
      "Fold5 size: (400, 37, 50, 1) (400, 50)\n"
     ]
    }
   ],
   "source": [
    "n_labels = 50 #no. of classes\n",
    "n_channels = 1 #intensity\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    \"\"\"Reformats to appropriate shape for covnet. Use 1-hot encoding for labels\"\"\"\n",
    "    dataset = dataset.reshape((-1, image_height, image_width, n_channels)).astype(np.float32)\n",
    "    labels = (np.arange(n_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "fold1, fold1_L = reformat(fold1, fold1_L)\n",
    "fold2, fold2_L = reformat(fold2, fold2_L)\n",
    "fold3, fold3_L = reformat(fold3, fold3_L)\n",
    "fold4, fold4_L = reformat(fold4, fold4_L)\n",
    "fold5, fold5_L = reformat(fold5, fold5_L)\n",
    "print('Fold1 size:', fold1.shape, fold1_L.shape)\n",
    "print('Fold2 size:', fold2.shape, fold2_L.shape)\n",
    "print('Fold3 size:', fold3.shape, fold3_L.shape)\n",
    "print('Fold4 size:', fold4.shape, fold4_L.shape)\n",
    "print('Fold5 size:', fold5.shape, fold5_L.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train and test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_queue = tf.train.slice_input_producer([fold1, fold1_L],shuffle=True)\n",
    "# image_batch, label_batch = tf.train.batch([input_queue[0], input_queue[1]],\n",
    "#                                           batch_size=batch_size)\n",
    "# with tf.Session() as sess:\n",
    "  \n",
    "#   # initialize the variables\n",
    "#     tf.global_variables_initializer().run()\n",
    "  \n",
    "#     a = sess.run(input_queue)\n",
    "#     print(a)\n",
    "big_fold = np.empty([2000,37,50,1])\n",
    "big_fold[0:400,:,:,:]=fold1\n",
    "big_fold[400:800,:,:,:]=fold2\n",
    "big_fold[800:1200,:,:,:]=fold3\n",
    "big_fold[1200:1600,:,:,:]=fold4\n",
    "big_fold[1600:2000,:,:,:]=fold5\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd() #current working folder\n",
    "\n",
    "# Parameters\n",
    "#if want to use gradient descent tune below else use different optimizer\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.05\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.96, staircase=True)\n",
    "#learning_rate = 0.01\n",
    "batch_size = 16\n",
    "num_steps = 101\n",
    "display_step = 10\n",
    "graph_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "dropout = .5 # Dropout, probability to keep units\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    #w_h = tf.summary.histogram(\"conv1\",conv1)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    #conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    #conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    #conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    #conv3 = maxpool2d(conv3, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv1, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    #fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])\n",
    "    #fc2 = tf.nn.relu(fc2)\n",
    "    # Apply Dropout\n",
    "    #fc2 = tf.nn.dropout(fc2, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 24 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([37, 5, 1, 128], stddev=0.1)),\n",
    "    # 5x5 conv, 24 inputs, 48 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([19, 5, 128, 180], stddev=0.1)),\n",
    "    # 5x5 conv, 48 inputs, 48 outputs\n",
    "    'wc3': tf.Variable(tf.truncated_normal([5, 5, 48, 48], stddev=0.1)),\n",
    "    # fully connected, (1025//4=257)*(196//4=49)*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([10*13*180, 1200], stddev=0.1)),\n",
    "    # fully connected, (1025//4=257)*(196//4=49)*64 inputs, 1024 outputs\n",
    "    'wd2': tf.Variable(tf.truncated_normal([1000, 1000], stddev=0.1)),\n",
    "    # 1024 inputs, 50 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1200, n_labels], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([128])),\n",
    "    'bc2': tf.Variable(tf.constant(1.0,shape=[180])),\n",
    "    'bc3': tf.Variable(tf.constant(1.0,shape=[48])),\n",
    "    'bd1': tf.Variable(tf.constant(1.0,shape=[1200])),\n",
    "    'bd2': tf.Variable(tf.constant(1.0,shape=[1200])),\n",
    "    'out': tf.Variable(tf.constant(1.0,shape=[n_labels]))\n",
    "}\n",
    "       \n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "      \n",
    "# Define cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    \n",
    "# Optimizer.\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "  \n",
    "# Predictions for the training, validation, and test data.\n",
    "prob = tf.nn.softmax(pred)\n",
    "#valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#test_prediction = tf.nn.softmax(conv_net(x, weights, biases, keep_prob))\n",
    "    \n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "accuracy = 100*tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "save_all = tf.train.Saver() #create saver handle\n",
    "#filter_summary = tf.summary.image(\"conv1\",weights['wc1'])\n",
    "#w_h = tf.summary.histogram(\"weights\", tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1']))\n",
    "#b_h = tf.summary.histogram(\"biases\", biases['bd1'])\n",
    "cost_h = tf.summary.scalar(\"cost_function\", cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6981bb5358b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         l, acc, train_summary = session.run([cost, accuracy, merged],\n\u001b[0;32m---> 20\u001b[0;31m                                                 feed_dict= {x:image_batch, y:label_batch,keep_prob:dropout})\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;31m#_, l, acc = session.run([optimizer, cost, accuracy],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m    926\u001b[0m                             \u001b[1;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                             'strings, lists, or numpy ndarrays.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays."
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "accs = []\n",
    "ls = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(cwd + '/results/stft/train',session.graph)\n",
    "    test_writer = tf.summary.FileWriter(cwd + '/results/stft/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data = train_set[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "\n",
    "        l, acc, train_summary = session.run([cost, accuracy, merged],\n",
    "                                                feed_dict= {x:batch_data, y:batch_labels,keep_prob:dropout})\n",
    "        \n",
    "        #_, l, acc = session.run([optimizer, cost, accuracy],\n",
    "        #                        feed_dict= {x:batch_data, y:batch_labels,keep_prob:dropout})\n",
    "        if (step % display_step == 0):\n",
    "            train_writer.add_summary(train_summary, step)\n",
    "            print('loss at step %d: %f' % (step, l))\n",
    "            print('training accuracy: %.1f%%' % acc)\n",
    "        \n",
    "        if (step % graph_step == 0):\n",
    "            steps.append(step)\n",
    "            accs.append(acc)\n",
    "            ls.append(l)\n",
    "    \n",
    "    #save_all.save(session, cwd + '/model3/data-all.chkp') #save model\n",
    "    elapsed_time = time.monotonic() - start_time\n",
    "    print(\"Training time taken:\",time_taken(elapsed_time))\n",
    "        \n",
    "    test_acc, test_summary = session.run([accuracy,merged], \n",
    "                                    feed_dict={x:test_set, y:test_label, keep_prob:1.}) #run model on test set\n",
    "    test_writer.add_summary(test_summary)\n",
    "    print(\"Testing Accuracy:\",test_acc,\"%\")\n",
    "        \n",
    "    #Plot graph of accuracy and loss against no. of steps\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('steps')\n",
    "    ax1.set_ylabel('accuracy', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.plot(steps,accs,'b-')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(steps,ls,'r-')\n",
    "    ax2.set_ylabel('loss', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    ax1.plot(step,test_acc,'go')\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('C:/Users/Huz/Documents/python_scripts/ESC50/results/kernel_size/test2.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF CODE (IGNORE BELOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "num_steps = 500\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "      return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "              / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    k_fold = KFold(n_splits=n_splits,shuffle=True)\n",
    "    for train_ind, valid_ind in k_fold.split(train_set):\n",
    "        batch_data = train_set[train_ind, :, :, :]\n",
    "        batch_label = train_label[train_ind, :]\n",
    "        valid_data = train_set[valid_ind, :, :, :]\n",
    "        valid_label = train_label[valid_ind, :]\n",
    "        for step in range(num_steps):\n",
    "            #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            #batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            #batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_label, tf_valid_dataset:valid_data}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 50 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_label))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_label))\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score   \n",
    "    \n",
    "k_fold = KFold(n_splits=4,shuffle=True)\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(train_set):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    #print(test_indices.shape)\n",
    "    break\n",
    "\n",
    "for train_ind, valid_ind in k_fold.split(train_set):\n",
    "    batch_data = train_set[train_ind, :, :, :]\n",
    "    batch_label = train_label[train_ind, :]\n",
    "    valid_data = train_set[valid_ind, :, :, :]\n",
    "    valid_label = train_label[valid_ind, :]\n",
    "    print(batch_data.shape)\n",
    "    print(batch_label.shape)\n",
    "    print(valid_data.shape)\n",
    "    print(valid_label.shape)\n",
    "    #test_set = np.array([data_files[data_class][i] for i in test_indices])  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
